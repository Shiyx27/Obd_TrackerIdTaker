{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10992138,"sourceType":"datasetVersion","datasetId":6841907}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shiyamaladevirs/final-obdtrackerid?scriptVersionId=226988534\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T11:12:52.378753Z","iopub.execute_input":"2025-03-11T11:12:52.379119Z","iopub.status.idle":"2025-03-11T11:12:52.387199Z","shell.execute_reply.started":"2025-03-11T11:12:52.379093Z","shell.execute_reply":"2025-03-11T11:12:52.386284Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/demodataset/Week 10 NikhilkrishnanOBD1.xlsx\n/kaggle/input/demodataset/Copy of OBD Sensitive Verticals _ A (1) (1).xlsx\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"**PLEASE USE GPU T4 x2 For FASTER RESULTS**","metadata":{}},{"cell_type":"code","source":"!pip install XlsxWriter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T11:12:55.101074Z","iopub.execute_input":"2025-03-11T11:12:55.101368Z","iopub.status.idle":"2025-03-11T11:12:58.612163Z","shell.execute_reply.started":"2025-03-11T11:12:55.101346Z","shell.execute_reply":"2025-03-11T11:12:58.611175Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: XlsxWriter in /usr/local/lib/python3.10/dist-packages (3.2.2)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import time\nimport os\nimport re\nimport csv\nimport pandas as pd\nimport numpy as np\n\n# Try to import cuDF for GPU acceleration; if not available, fall back to Dask.\ntry:\n    import cudf\n    gpu_available = True\n    print(\"GPU acceleration available: Using RAPIDS cuDF\")\nexcept ImportError:\n    gpu_available = False\n    import dask.dataframe as dd\n    print(\"GPU acceleration not available: Using Dask\")\n\ndef read_xlsx_as_dataframe(file):\n    \"\"\"\n    Reads an XLSX file using openpyxl in read-only mode\n    and converts the data to a pandas DataFrame.\n    \"\"\"\n    from openpyxl import load_workbook\n    wb = load_workbook(filename=file, read_only=True, data_only=True)\n    ws = wb.active\n    data = list(ws.values)\n    # Assume first row contains column headers.\n    columns = data[0]\n    df = pd.DataFrame(data[1:], columns=columns)\n    return df\n\ndef load_main_file(main_file, npartitions=4):\n    \"\"\"\n    Load the main file (CSV or XLSX) as a DataFrame.\n    Uses cuDF if GPU is available; otherwise, uses Dask.\n    For XLSX files, uses openpyxl's read-only mode to improve performance.\n    \"\"\"\n    ext = os.path.splitext(main_file)[1].lower()\n    if gpu_available:\n        if ext == '.csv':\n            print(\"Reading main CSV file with cuDF on GPU...\")\n            df = cudf.read_csv(main_file)\n        elif ext in ['.xlsx', '.xls']:\n            print(\"Reading main XLSX file using openpyxl (read-only mode) and converting to cuDF on GPU...\")\n            df_pd = read_xlsx_as_dataframe(main_file)\n            # Convert object columns to string to avoid MixedTypeError.\n            for col in df_pd.select_dtypes(include=['object']).columns:\n                df_pd[col] = df_pd[col].astype(str)\n            df = cudf.DataFrame.from_pandas(df_pd)\n        else:\n            raise ValueError(f\"Unsupported file format: {ext}\")\n    else:\n        if ext == '.csv':\n            print(\"Reading main CSV file with Dask...\")\n            df = dd.read_csv(\n                main_file,\n                engine='python',\n                sep=',',\n                quoting=csv.QUOTE_NONE,\n                escapechar='\\\\',\n                on_bad_lines='skip',\n                encoding='latin1',\n                assume_missing=True,\n                blocksize=\"100MB\"\n            )\n        elif ext in ['.xlsx', '.xls']:\n            print(\"Reading main XLSX file using openpyxl (read-only mode) and converting to Dask DataFrame...\")\n            df_pd = read_xlsx_as_dataframe(main_file)\n            df = dd.from_pandas(df_pd, npartitions=npartitions)\n        else:\n            raise ValueError(f\"Unsupported file format: {ext}\")\n    return df\n\ndef load_obd_sensitive(obd_file):\n    \"\"\"\n    Load OBD sensitive verticals from an Excel file.\n    If the 'vertical' column is missing, the first column is used.\n    Returns a set of normalized sensitive keywords.\n    \"\"\"\n    obd_df = pd.read_excel(obd_file, engine='openpyxl')\n    if 'vertical' not in obd_df.columns:\n        print(\"Column 'vertical' not found in OBD file. Using the first column as 'vertical'.\")\n        first_col = obd_df.columns[0]\n        obd_df['vertical'] = obd_df[first_col]\n    obd_df['vertical'] = obd_df['vertical'].astype(str).str.strip().str.lower()\n    return set(obd_df['vertical'].dropna())\n\ndef ensure_columns(pdf):\n    \"\"\"\n    Ensure required columns exist in each partition.\n    \"\"\"\n    if 'hub_zone' not in pdf.columns:\n        pdf['hub_zone'] = \"unknown\"\n    return pdf\n\ndef process_dataframe(df, combined_sensitive):\n    \"\"\"\n    Process the dataframe:\n      - It first creates a temporary combined column (from brand, product_title, vertical)\n        to filter out rows matching sensitive keywords.\n      - It captures the removed rows (i.e. those matching OBD sensitive verticals).\n      - Then it further filters out rows if hub_name contains \"myntra\" or hub_type contains \"kirana\".\n      - It also fills missing 'parent_lm_hub' and hub_zone values.\n    Returns a tuple:\n      (filtered_df, removed_obd_sensitive_df, filtering_stats)\n    where filtering_stats is a dictionary with counts of rows before/after filtering and per filter.\n    \"\"\"\n    # Strip whitespace from column names but preserve original case\n    df.columns = df.columns.str.strip()\n    \n    # For Dask, ensure every partition has the necessary columns.\n    if not gpu_available:\n        df = df.map_partitions(ensure_columns)\n    \n    # Strip whitespace for key text columns (without converting to lower-case)\n    text_cols = ['brand', 'product_title', 'vertical', 'hub_name', 'parent_lm_hub', 'hub_zone', 'hub_type']\n    for col in text_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(str).str.strip()\n    \n    if not gpu_available:\n        df = df.reset_index(drop=True)\n    \n    # Record initial row count.\n    rows_before = len(df) if gpu_available else df.shape[0].compute()\n    \n    # -------------------------------\n    # OBD Sensitive Filtering\n    # -------------------------------\n    removed_obd_count = 0\n    removed_obd_sensitive = None\n    required_cols = ['brand', 'product_title', 'vertical']\n    if all(c in df.columns for c in required_cols):\n        # Create temporary combined column (in lower-case)\n        df['temp_combined'] = (df['brand'].fillna('') + \" \" + \n                                 df['product_title'].fillna('') + \" \" + \n                                 df['vertical'].fillna('')).str.lower().str.strip()\n        pattern = '|'.join([re.escape(word) for word in combined_sensitive])\n        if gpu_available:\n            mask_sensitive = df['temp_combined'].str.contains(pattern)\n            removed_obd_sensitive = df[mask_sensitive].copy()\n            removed_obd_count = len(removed_obd_sensitive)\n            df = df[~mask_sensitive]\n        else:\n            mask_sensitive = df['temp_combined'].str.contains(pattern, case=False, na=False)\n            removed_obd_sensitive = df[mask_sensitive].copy()\n            removed_obd_count = df[mask_sensitive].shape[0].compute()\n            df = df[~mask_sensitive]\n        df = df.drop(columns=['temp_combined'])\n    else:\n        removed_obd_sensitive = None\n\n    # -------------------------------\n    # Filter out rows where hub_name contains \"myntra\"\n    # -------------------------------\n    removed_myntra_count = 0\n    if 'hub_name' in df.columns:\n        if gpu_available:\n            mask_myntra = df['hub_name'].fillna('').str.lower().str.contains(\"myntra\", regex=False)\n            removed_myntra_count = len(df[mask_myntra])\n            df = df[~mask_myntra]\n        else:\n            mask_myntra = df['hub_name'].str.contains(\"myntra\", case=False, na=False)\n            removed_myntra_count = df[mask_myntra].shape[0].compute()\n            df = df[~mask_myntra]\n    \n    # -------------------------------\n    # Filter out rows where hub_type contains \"kirana\"\n    # -------------------------------\n    removed_kirana_count = 0\n    if 'hub_type' in df.columns:\n        if gpu_available:\n            mask_kirana = df['hub_type'].fillna('').str.lower().str.contains(\"kirana\", regex=False)\n            removed_kirana_count = len(df[mask_kirana])\n            df = df[~mask_kirana]\n        else:\n            mask_kirana = df['hub_type'].str.contains(\"kirana\", case=False, na=False)\n            removed_kirana_count = df[mask_kirana].shape[0].compute()\n            df = df[~mask_kirana]\n    \n    rows_after = len(df) if gpu_available else df.shape[0].compute()\n    \n    # -------------------------------\n    # Fill missing values for parent_lm_hub and hub_zone\n    # -------------------------------\n    if 'hub_name' in df.columns and 'parent_lm_hub' in df.columns:\n        df['parent_lm_hub'] = df['parent_lm_hub'].replace(\n            {\"\": np.nan, \"unknown\": np.nan, \"null\": np.nan, \"nan\": np.nan, \"n/a\": np.nan}\n        ).fillna(df['hub_name'])\n    \n    def fill_hub_zone(series):\n        valid = series[~series.isin([\"\", \"unknown\", \"null\", \"nan\", \"n/a\"])]\n        fill_value = valid.iloc[0] if not valid.empty else \"unknown\"\n        return series.fillna(fill_value).replace([\"\", \"unknown\", \"null\", \"nan\", \"n/a\"], fill_value)\n    \n    if 'hub_name' in df.columns and 'hub_zone' in df.columns:\n        if gpu_available:\n            df_pd = df.to_pandas()\n            df_pd['hub_zone'] = df_pd.groupby('hub_name')['hub_zone'].transform(fill_hub_zone)\n            df = cudf.DataFrame.from_pandas(df_pd)\n        else:\n            df = df.shuffle(on=\"hub_name\")\n            df['hub_zone'] = df.groupby('hub_name')['hub_zone'].transform(fill_hub_zone)\n    \n    filtering_stats = {\n        \"rows_before\": rows_before,\n        \"rows_after\": rows_after,\n        \"removed_obd_sensitive\": removed_obd_count,\n        \"removed_myntra\": removed_myntra_count,\n        \"removed_kirana\": removed_kirana_count,\n        \"total_removed\": rows_before - rows_after\n    }\n    \n    return df, removed_obd_sensitive, filtering_stats\n\ndef sample_per_hub(df, n=5):\n    \"\"\"\n    Sample up to n rows per 'parent_lm_hub' group.\n    If 'parent_lm_hub' does not exist, fallback to grouping by 'hub_name'.\n    \"\"\"\n    def sample_group(pdf, n=n):\n        return pdf.sample(n=n, random_state=42) if len(pdf) > n else pdf\n    \n    if 'parent_lm_hub' in df.columns:\n        group_col = 'parent_lm_hub'\n    elif 'hub_name' in df.columns:\n        group_col = 'hub_name'\n    else:\n        if gpu_available:\n            return df.to_pandas()\n        else:\n            return df.compute()\n    \n    if gpu_available:\n        df_pd = df.to_pandas()\n        sampled_pd = df_pd.groupby(group_col).apply(lambda x: sample_group(x, n)).reset_index(drop=True)\n        return sampled_pd\n    else:\n        sampled_ddf = df.groupby(group_col).apply(sample_group, meta=df._meta)\n        final_df = sampled_ddf.compute()\n        return final_df\n\ndef main(main_file, obd_file, output_file):\n    overall_start = time.time()\n    \n    # Load OBD sensitive keywords.\n    start = time.time()\n    obd_verticals = load_obd_sensitive(obd_file)\n    manual_sensitive = {'pampers', 'diaper'}  # Additional keywords.\n    combined_sensitive = obd_verticals.union(manual_sensitive)\n    print(\"Combined sensitive keywords:\", combined_sensitive)\n    print(\"Time to load OBD sensitive file: {:.2f} seconds\".format(time.time() - start))\n    \n    # Load main data.\n    start = time.time()\n    df_loaded = load_main_file(main_file)\n    print(\"Time to load main file: {:.2f} seconds\".format(time.time() - start))\n    \n    # Compute distinct hubs count before filtering.\n    if 'parent_lm_hub' in df_loaded.columns:\n        if gpu_available:\n            distinct_hubs_before = df_loaded['parent_lm_hub'].nunique()\n        else:\n            distinct_hubs_before = df_loaded['parent_lm_hub'].nunique().compute()\n    elif 'hub_name' in df_loaded.columns:\n        if gpu_available:\n            distinct_hubs_before = df_loaded['hub_name'].nunique()\n        else:\n            distinct_hubs_before = df_loaded['hub_name'].nunique().compute()\n    else:\n        distinct_hubs_before = \"N/A\"\n    \n    # Process the dataframe (filtering).\n    start = time.time()\n    df_filtered, removed_obd_sensitive, filtering_stats = process_dataframe(df_loaded, combined_sensitive)\n    print(\"Time to process dataframe: {:.2f} seconds\".format(time.time() - start))\n    \n    # Compute distinct hubs count after filtering.\n    if 'parent_lm_hub' in df_filtered.columns:\n        if gpu_available:\n            distinct_hubs_after = df_filtered['parent_lm_hub'].nunique()\n        else:\n            distinct_hubs_after = df_filtered['parent_lm_hub'].nunique().compute()\n    elif 'hub_name' in df_filtered.columns:\n        if gpu_available:\n            distinct_hubs_after = df_filtered['hub_name'].nunique()\n        else:\n            distinct_hubs_after = df_filtered['hub_name'].nunique().compute()\n    else:\n        distinct_hubs_after = \"N/A\"\n    \n    # Update overview stats.\n    overview_stats = {\n        \"Total rows before filtering\": filtering_stats[\"rows_before\"],\n        \"Total rows after filtering\": filtering_stats[\"rows_after\"],\n        \"Total rows removed\": filtering_stats[\"total_removed\"],\n        \"Removed due to OBD sensitive\": filtering_stats[\"removed_obd_sensitive\"],\n        \"Removed due to myntra\": filtering_stats[\"removed_myntra\"],\n        \"Removed due to kirana\": filtering_stats[\"removed_kirana\"],\n        \"Distinct hubs before filtering\": distinct_hubs_before,\n        \"Distinct hubs after filtering\": distinct_hubs_after\n    }\n    \n    # Sample final data per hub.\n    start = time.time()\n    final_df = sample_per_hub(df_filtered, n=5)  # Sampling 5 rows per parent_lm_hub group.\n    print(\"Time to sample per hub: {:.2f} seconds\".format(time.time() - start))\n    \n    # Write to Excel with multiple sheets.\n    start = time.time()\n    with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n        # Write the final tracking data.\n        final_df.to_excel(writer, sheet_name=\"TrackingIDs\", index=False)\n        # Write the removed OBD sensitive rows.\n        if removed_obd_sensitive is not None:\n            if gpu_available:\n                removed_obd_df = removed_obd_sensitive.to_pandas()\n            else:\n                removed_obd_df = removed_obd_sensitive.compute()\n            removed_obd_df.to_excel(writer, sheet_name=\"OBD Removed\", index=False)\n        else:\n            # If nothing was removed, write an empty DataFrame.\n            pd.DataFrame().to_excel(writer, sheet_name=\"OBD Removed\", index=False)\n        # Write the overview statistics.\n        overview_df = pd.DataFrame(list(overview_stats.items()), columns=[\"Metric\", \"Value\"])\n        overview_df.to_excel(writer, sheet_name=\"Overview\", index=False)\n    print(\"Time to write Excel: {:.2f} seconds\".format(time.time() - start))\n    \n    overall_time = time.time() - overall_start\n    print(\"Total time taken: {:.2f} seconds\".format(overall_time))\n    print(\"Done. Output written to\", output_file)\n\nif __name__ == '__main__':\n    MAIN_FILE = \"/kaggle/input/demodataset/Week 10 NikhilkrishnanOBD1.xlsx\"  # or .xlsx file\n    OBD_FILE = \"/kaggle/input/demodataset/Copy of OBD Sensitive Verticals _ A (1) (1).xlsx\"\n    OUTPUT_FILE = \"/kaggle/working/processed_data.xlsx\"\n    main(MAIN_FILE, OBD_FILE, OUTPUT_FILE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T11:13:35.146026Z","iopub.execute_input":"2025-03-11T11:13:35.146384Z","iopub.status.idle":"2025-03-11T11:17:22.334555Z","shell.execute_reply.started":"2025-03-11T11:13:35.146359Z","shell.execute_reply":"2025-03-11T11:17:22.333725Z"}},"outputs":[{"name":"stdout","text":"GPU acceleration available: Using RAPIDS cuDF\nColumn 'vertical' not found in OBD file. Using the first column as 'vertical'.\nCombined sensitive keywords: {'menstrunk', 'mensvest', 'sexualmassager', 'breastnipplecare', 'boysleepwear', 'adultdiapers', 'breastpump', 'girlsleepwear', 'femaledisorders', 'pregnancykit', 'tampon', 'womenboxer', 'womennightdressnighty', 'pantyliner', 'girlinnerwear', 'womenlingerieset', 'mensswimsuit', 'pampers', 'boyinnerwear', 'girlswimsuit', 'condom', 'feedingnursing', 'sanitarypad', 'fertilitysupplement', 'babybooty', 'womenswimsuit', 'womencamisoleslip', 'femaleurinationdevice', 'womenbra', 'infantbodysuit', 'infantinnerwear', 'sexualhealth', 'womenintimatecare', 'womennightsuit', 'womenpanty', 'girlblouse', 'mensbrief', 'womenshirttoptunic', 'infantsleepwear', 'womensportbra', 'roleplaytoy', 'mensboxer', 'fertilitykit', 'womenbabydoll', 'menstrualcups', 'pleasureenhancement', 'maledisorders', 'womenshapewear', 'girlbodysuit', 'sexualcomboandkit', 'diaper', 'mentalwellnessproducts', 'piles', 'womenblouse'}\nTime to load OBD sensitive file: 0.27 seconds\nReading main XLSX file using openpyxl (read-only mode) and converting to cuDF on GPU...\nTime to load main file: 176.78 seconds\nTime to process dataframe: 13.69 seconds\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-6-3f26c00d08fd>:236: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  sampled_pd = df_pd.groupby(group_col).apply(lambda x: sample_group(x, n)).reset_index(drop=True)\n","output_type":"stream"},{"name":"stdout","text":"Time to sample per hub: 5.41 seconds\nTime to write Excel: 30.93 seconds\nTotal time taken: 227.10 seconds\nDone. Output written to /kaggle/working/processed_data.xlsx\n","output_type":"stream"}],"execution_count":6}]}