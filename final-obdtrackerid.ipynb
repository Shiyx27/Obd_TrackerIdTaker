{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shiyamaladevirs/final-obdtrackerid?scriptVersionId=226979384\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"fa5e4fec","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-03-11T10:22:55.082303Z","iopub.status.busy":"2025-03-11T10:22:55.081955Z","iopub.status.idle":"2025-03-11T10:22:56.287875Z","shell.execute_reply":"2025-03-11T10:22:56.286959Z"},"papermill":{"duration":1.211701,"end_time":"2025-03-11T10:22:56.289398","exception":false,"start_time":"2025-03-11T10:22:55.077697","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/demodataset/Week 10 NikhilkrishnanOBD1.xlsx\n","/kaggle/input/demodataset/Copy of OBD Sensitive Verticals _ A (1) (1).xlsx\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"8d1f49b7","metadata":{"papermill":{"duration":0.001565,"end_time":"2025-03-11T10:22:56.293172","exception":false,"start_time":"2025-03-11T10:22:56.291607","status":"completed"},"tags":[]},"source":["**PLEASE USE GPU T4 x2 For FASTER RESULTS**"]},{"cell_type":"code","execution_count":2,"id":"451f1bc3","metadata":{"execution":{"iopub.execute_input":"2025-03-11T10:22:56.297622Z","iopub.status.busy":"2025-03-11T10:22:56.297238Z","iopub.status.idle":"2025-03-11T10:27:25.06281Z","shell.execute_reply":"2025-03-11T10:27:25.061842Z"},"papermill":{"duration":268.770148,"end_time":"2025-03-11T10:27:25.064956","exception":false,"start_time":"2025-03-11T10:22:56.294808","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting XlsxWriter\r\n","  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\r\n","Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: XlsxWriter\r\n","Successfully installed XlsxWriter-3.2.2\r\n","GPU acceleration available: Using RAPIDS cuDF\n","Column 'vertical' not found in OBD file. Using the first column as 'vertical'.\n","Combined sensitive keywords: {'tampon', 'womensportbra', 'babybooty', 'girlblouse', 'femaleurinationdevice', 'girlbodysuit', 'maledisorders', 'menstrualcups', 'infantbodysuit', 'roleplaytoy', 'girlswimsuit', 'mensvest', 'adultdiapers', 'pleasureenhancement', 'fertilitykit', 'mentalwellnessproducts', 'condom', 'diaper', 'womenblouse', 'mensboxer', 'girlsleepwear', 'sexualcomboandkit', 'mensbrief', 'breastpump', 'womenboxer', 'infantsleepwear', 'womenpanty', 'femaledisorders', 'mensswimsuit', 'girlinnerwear', 'pregnancykit', 'sexualhealth', 'womennightsuit', 'pampers', 'womenbra', 'feedingnursing', 'womenlingerieset', 'womenshapewear', 'infantinnerwear', 'womenbabydoll', 'womencamisoleslip', 'menstrunk', 'boysleepwear', 'pantyliner', 'sexualmassager', 'womennightdressnighty', 'womenshirttoptunic', 'fertilitysupplement', 'womenintimatecare', 'sanitarypad', 'womenswimsuit', 'boyinnerwear', 'breastnipplecare', 'piles'}\n","Time to load OBD sensitive file: 0.96 seconds\n","Reading main XLSX file with pandas and converting to cuDF on GPU...\n","Time to load main file: 217.30 seconds\n","Total rows before filtering sensitive content: 667375\n","Rows after filtering sensitive content: 612774\n","Time to process dataframe: 13.66 seconds\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-2-47918b69b495>:158: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  sampled_pd = df_pd.groupby('hub_name').apply(lambda x: sample_group(x, n)).reset_index(drop=True)\n"]},{"name":"stdout","output_type":"stream","text":["Time to sample per hub: 5.74 seconds\n","Time to write Excel: 17.12 seconds\n","Total time taken: 254.78 seconds\n","Done. Output written to /kaggle/working/processed_data.xlsx\n"]}],"source":["!pip install XlsxWriter\n","\n","import time\n","import os\n","import re\n","import csv\n","import pandas as pd\n","import numpy as np\n","\n","# Try to import cuDF for GPU acceleration; if not available, fall back to Dask.\n","try:\n","    import cudf\n","    gpu_available = True\n","    print(\"GPU acceleration available: Using RAPIDS cuDF\")\n","except ImportError:\n","    gpu_available = False\n","    import dask.dataframe as dd\n","    print(\"GPU acceleration not available: Using Dask\")\n","\n","def load_main_file(main_file, npartitions=4):\n","    \"\"\"\n","    Load the main file (CSV or XLSX) as a DataFrame.\n","    Uses cuDF if GPU is available; otherwise, uses Dask.\n","    \"\"\"\n","    ext = os.path.splitext(main_file)[1].lower()\n","    if gpu_available:\n","        if ext == '.csv':\n","            print(\"Reading main CSV file with cuDF on GPU...\")\n","            df = cudf.read_csv(main_file)\n","        elif ext in ['.xlsx', '.xls']:\n","            print(\"Reading main XLSX file with pandas and converting to cuDF on GPU...\")\n","            df_pd = pd.read_excel(main_file, engine='openpyxl')\n","            # Convert all object columns to string to avoid MixedTypeError\n","            for col in df_pd.select_dtypes(include=['object']).columns:\n","                df_pd[col] = df_pd[col].astype(str)\n","            df = cudf.DataFrame.from_pandas(df_pd)\n","        else:\n","            raise ValueError(f\"Unsupported file format: {ext}\")\n","    else:\n","        if ext == '.csv':\n","            print(\"Reading main CSV file with Dask...\")\n","            df = dd.read_csv(\n","                main_file,\n","                engine='python',\n","                sep=',',\n","                quoting=csv.QUOTE_NONE,\n","                escapechar='\\\\',\n","                on_bad_lines='skip',\n","                encoding='latin1',\n","                assume_missing=True,\n","                blocksize=\"100MB\"\n","            )\n","        elif ext in ['.xlsx', '.xls']:\n","            print(\"Reading main XLSX file with pandas and converting to Dask DataFrame...\")\n","            df_pd = pd.read_excel(main_file, engine='openpyxl')\n","            df = dd.from_pandas(df_pd, npartitions=npartitions)\n","        else:\n","            raise ValueError(f\"Unsupported file format: {ext}\")\n","    return df\n","\n","def load_obd_sensitive(obd_file):\n","    \"\"\"\n","    Load OBD sensitive verticals from an Excel file.\n","    If the 'vertical' column is missing, the first column is used.\n","    Returns a set of normalized sensitive keywords.\n","    \"\"\"\n","    obd_df = pd.read_excel(obd_file, engine='openpyxl')\n","    if 'vertical' not in obd_df.columns:\n","        print(\"Column 'vertical' not found in OBD file. Using the first column as 'vertical'.\")\n","        first_col = obd_df.columns[0]\n","        obd_df['vertical'] = obd_df[first_col]\n","    obd_df['vertical'] = obd_df['vertical'].astype(str).str.strip().str.lower()\n","    return set(obd_df['vertical'].dropna())\n","\n","def ensure_columns(pdf):\n","    \"\"\"\n","    Ensure required columns exist in each partition.\n","    \"\"\"\n","    if 'hub_zone' not in pdf.columns:\n","        pdf['hub_zone'] = \"unknown\"\n","    return pdf\n","\n","def process_dataframe(df, combined_sensitive):\n","    \"\"\"\n","    Process the dataframe while preserving original text case.\n","    For filtering, a temporary lower-case combined column is created,\n","    then dropped so that original capitalization is retained.\n","    \"\"\"\n","    # Strip whitespace from column names but preserve original case\n","    df.columns = df.columns.str.strip()\n","    \n","    # For Dask, ensure every partition has the necessary columns.\n","    if not gpu_available:\n","        df = df.map_partitions(ensure_columns)\n","    \n","    # Strip whitespace for key text columns (without converting to lower-case)\n","    text_cols = ['brand', 'product_title', 'vertical', 'hub_name', 'parent_lm_hub', 'hub_zone']\n","    for col in text_cols:\n","        if col in df.columns:\n","            df[col] = df[col].astype(str).str.strip()\n","    \n","    if not gpu_available:\n","        df = df.reset_index(drop=True)\n","    \n","    total_rows_before = len(df) if gpu_available else df.shape[0].compute()\n","    print(\"Total rows before filtering sensitive content:\", total_rows_before)\n","    \n","    # For filtering, create a temporary combined column with lower-case conversion\n","    required_cols = ['brand', 'product_title', 'vertical']\n","    if all(c in df.columns for c in required_cols):\n","        df['temp_combined'] = (df['brand'].fillna('') + \" \" + \n","                                 df['product_title'].fillna('') + \" \" + \n","                                 df['vertical'].fillna('')).str.lower().str.strip()\n","        \n","        pattern = '|'.join([re.escape(word) for word in combined_sensitive])\n","        if gpu_available:\n","            mask = ~df['temp_combined'].str.contains(pattern)\n","            df = df[mask]\n","        else:\n","            df = df[~df['temp_combined'].str.contains(pattern, case=False, na=False)]\n","        df = df.drop(columns=['temp_combined'])\n","    \n","    filtered_count = len(df) if gpu_available else df.shape[0].compute()\n","    print(\"Rows after filtering sensitive content:\", filtered_count)\n","    \n","    # Fill missing 'parent_lm_hub' with 'hub_name' (preserving original case)\n","    if 'hub_name' in df.columns and 'parent_lm_hub' in df.columns:\n","        df['parent_lm_hub'] = df['parent_lm_hub'].replace(\n","            {\"\": np.nan, \"unknown\": np.nan, \"null\": np.nan, \"nan\": np.nan, \"n/a\": np.nan}\n","        ).fillna(df['hub_name'])\n","    \n","    # Fill missing 'hub_zone' values within each 'hub_name' group.\n","    def fill_hub_zone(series):\n","        valid = series[~series.isin([\"\", \"unknown\", \"null\", \"nan\", \"n/a\"])]\n","        fill_value = valid.iloc[0] if not valid.empty else \"unknown\"\n","        return series.fillna(fill_value).replace([\"\", \"unknown\", \"null\", \"nan\", \"n/a\"], fill_value)\n","    \n","    if 'hub_name' in df.columns and 'hub_zone' in df.columns:\n","        if gpu_available:\n","            df_pd = df.to_pandas()\n","            df_pd['hub_zone'] = df_pd.groupby('hub_name')['hub_zone'].transform(fill_hub_zone)\n","            df = cudf.DataFrame.from_pandas(df_pd)\n","        else:\n","            df = df.shuffle(on=\"hub_name\")\n","            df['hub_zone'] = df.groupby('hub_name')['hub_zone'].transform(fill_hub_zone)\n","    \n","    return df\n","\n","def sample_per_hub(df, n=10):\n","    \"\"\"\n","    Sample up to n rows per 'hub_name' group.\n","    \"\"\"\n","    def sample_group(pdf, n=n):\n","        return pdf.sample(n=n, random_state=42) if len(pdf) > n else pdf\n","    if 'hub_name' in df.columns:\n","        if gpu_available:\n","            df_pd = df.to_pandas()\n","            sampled_pd = df_pd.groupby('hub_name').apply(lambda x: sample_group(x, n)).reset_index(drop=True)\n","            return sampled_pd\n","        else:\n","            sampled_ddf = df.groupby('hub_name').apply(sample_group, meta=df._meta)\n","            final_df = sampled_ddf.compute()\n","            return final_df\n","    else:\n","        if gpu_available:\n","            return df.to_pandas()\n","        else:\n","            return df.compute()\n","\n","def main(main_file, obd_file, output_file):\n","    overall_start = time.time()\n","    \n","    start = time.time()\n","    obd_verticals = load_obd_sensitive(obd_file)\n","    manual_sensitive = {'pampers', 'diaper'}  # Additional keywords.\n","    combined_sensitive = obd_verticals.union(manual_sensitive)\n","    print(\"Combined sensitive keywords:\", combined_sensitive)\n","    print(\"Time to load OBD sensitive file: {:.2f} seconds\".format(time.time() - start))\n","    \n","    start = time.time()\n","    df = load_main_file(main_file)\n","    print(\"Time to load main file: {:.2f} seconds\".format(time.time() - start))\n","    \n","    start = time.time()\n","    df = process_dataframe(df, combined_sensitive)\n","    print(\"Time to process dataframe: {:.2f} seconds\".format(time.time() - start))\n","    \n","    start = time.time()\n","    final_df = sample_per_hub(df, n=10)\n","    print(\"Time to sample per hub: {:.2f} seconds\".format(time.time() - start))\n","    \n","    start = time.time()\n","    with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n","        final_df.to_excel(writer, sheet_name=\"TrackingIDs\", index=False)\n","        obd_df = pd.read_excel(obd_file, engine='openpyxl')\n","        obd_df.to_excel(writer, sheet_name=\"OBD Sensitive\", index=False)\n","    print(\"Time to write Excel: {:.2f} seconds\".format(time.time() - start))\n","    \n","    overall_time = time.time() - overall_start\n","    print(\"Total time taken: {:.2f} seconds\".format(overall_time))\n","    print(\"Done. Output written to\", output_file)\n","\n","if __name__ == '__main__':\n","    MAIN_FILE = \"/kaggle/input/demodataset/Week 10 NikhilkrishnanOBD1.xlsx\"  # Give THE Path of input file\n","    OBD_FILE = \"/kaggle/input/demodataset/Copy of OBD Sensitive Verticals _ A (1) (1).xlsx\" # Give THE Path of OBD Sensitive file\n","    OUTPUT_FILE = \"/kaggle/working/processed_data.xlsx\"\n","    main(MAIN_FILE, OBD_FILE, OUTPUT_FILE)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":6841907,"sourceId":10992138,"sourceType":"datasetVersion"}],"dockerImageVersionId":30918,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":276.319935,"end_time":"2025-03-11T10:27:27.991281","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-11T10:22:51.671346","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}