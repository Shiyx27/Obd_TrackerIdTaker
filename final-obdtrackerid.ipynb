{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shiyamaladevirs/final-obdtrackerid?scriptVersionId=226979890\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T10:07:59.227389Z","iopub.execute_input":"2025-03-11T10:07:59.22761Z","iopub.status.idle":"2025-03-11T10:08:00.002578Z","shell.execute_reply.started":"2025-03-11T10:07:59.22759Z","shell.execute_reply":"2025-03-11T10:08:00.001657Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"**PLEASE USE GPU T4 x2 For FASTER RESULTS**","metadata":{}},{"cell_type":"code","source":"!pip install XlsxWriter\n\nimport time\nimport os\nimport re\nimport csv\nimport pandas as pd\nimport numpy as np\n\n# Try to import cuDF for GPU acceleration; if not available, fall back to Dask.\ntry:\n    import cudf\n    gpu_available = True\n    print(\"GPU acceleration available: Using RAPIDS cuDF\")\nexcept ImportError:\n    gpu_available = False\n    import dask.dataframe as dd\n    print(\"GPU acceleration not available: Using Dask\")\n\ndef load_main_file(main_file, npartitions=4):\n    \"\"\"\n    Load the main file (CSV or XLSX) as a DataFrame.\n    Uses cuDF if GPU is available; otherwise, uses Dask.\n    \"\"\"\n    ext = os.path.splitext(main_file)[1].lower()\n    if gpu_available:\n        if ext == '.csv':\n            print(\"Reading main CSV file with cuDF on GPU...\")\n            df = cudf.read_csv(main_file)\n        elif ext in ['.xlsx', '.xls']:\n            print(\"Reading main XLSX file with pandas and converting to cuDF on GPU...\")\n            df_pd = pd.read_excel(main_file, engine='openpyxl')\n            # Convert all object columns to string to avoid MixedTypeError\n            for col in df_pd.select_dtypes(include=['object']).columns:\n                df_pd[col] = df_pd[col].astype(str)\n            df = cudf.DataFrame.from_pandas(df_pd)\n        else:\n            raise ValueError(f\"Unsupported file format: {ext}\")\n    else:\n        if ext == '.csv':\n            print(\"Reading main CSV file with Dask...\")\n            df = dd.read_csv(\n                main_file,\n                engine='python',\n                sep=',',\n                quoting=csv.QUOTE_NONE,\n                escapechar='\\\\',\n                on_bad_lines='skip',\n                encoding='latin1',\n                assume_missing=True,\n                blocksize=\"100MB\"\n            )\n        elif ext in ['.xlsx', '.xls']:\n            print(\"Reading main XLSX file with pandas and converting to Dask DataFrame...\")\n            df_pd = pd.read_excel(main_file, engine='openpyxl')\n            df = dd.from_pandas(df_pd, npartitions=npartitions)\n        else:\n            raise ValueError(f\"Unsupported file format: {ext}\")\n    return df\n\ndef load_obd_sensitive(obd_file):\n    \"\"\"\n    Load OBD sensitive verticals from an Excel file.\n    If the 'vertical' column is missing, the first column is used.\n    Returns a set of normalized sensitive keywords.\n    \"\"\"\n    obd_df = pd.read_excel(obd_file, engine='openpyxl')\n    if 'vertical' not in obd_df.columns:\n        print(\"Column 'vertical' not found in OBD file. Using the first column as 'vertical'.\")\n        first_col = obd_df.columns[0]\n        obd_df['vertical'] = obd_df[first_col]\n    obd_df['vertical'] = obd_df['vertical'].astype(str).str.strip().str.lower()\n    return set(obd_df['vertical'].dropna())\n\ndef ensure_columns(pdf):\n    \"\"\"\n    Ensure required columns exist in each partition.\n    \"\"\"\n    if 'hub_zone' not in pdf.columns:\n        pdf['hub_zone'] = \"unknown\"\n    return pdf\n\ndef process_dataframe(df, combined_sensitive):\n    \"\"\"\n    Process the dataframe while preserving original text case.\n    For filtering, a temporary lower-case combined column is created,\n    then dropped so that original capitalization is retained.\n    \"\"\"\n    # Strip whitespace from column names but preserve original case\n    df.columns = df.columns.str.strip()\n    \n    # For Dask, ensure every partition has the necessary columns.\n    if not gpu_available:\n        df = df.map_partitions(ensure_columns)\n    \n    # Strip whitespace for key text columns (without converting to lower-case)\n    text_cols = ['brand', 'product_title', 'vertical', 'hub_name', 'parent_lm_hub', 'hub_zone']\n    for col in text_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(str).str.strip()\n    \n    if not gpu_available:\n        df = df.reset_index(drop=True)\n    \n    total_rows_before = len(df) if gpu_available else df.shape[0].compute()\n    print(\"Total rows before filtering sensitive content:\", total_rows_before)\n    \n    # For filtering, create a temporary combined column with lower-case conversion\n    required_cols = ['brand', 'product_title', 'vertical']\n    if all(c in df.columns for c in required_cols):\n        df['temp_combined'] = (df['brand'].fillna('') + \" \" + \n                                 df['product_title'].fillna('') + \" \" + \n                                 df['vertical'].fillna('')).str.lower().str.strip()\n        \n        pattern = '|'.join([re.escape(word) for word in combined_sensitive])\n        if gpu_available:\n            mask = ~df['temp_combined'].str.contains(pattern)\n            df = df[mask]\n        else:\n            df = df[~df['temp_combined'].str.contains(pattern, case=False, na=False)]\n        df = df.drop(columns=['temp_combined'])\n    \n    filtered_count = len(df) if gpu_available else df.shape[0].compute()\n    print(\"Rows after filtering sensitive content:\", filtered_count)\n    \n    # Fill missing 'parent_lm_hub' with 'hub_name' (preserving original case)\n    if 'hub_name' in df.columns and 'parent_lm_hub' in df.columns:\n        df['parent_lm_hub'] = df['parent_lm_hub'].replace(\n            {\"\": np.nan, \"unknown\": np.nan, \"null\": np.nan, \"nan\": np.nan, \"n/a\": np.nan}\n        ).fillna(df['hub_name'])\n    \n    # Fill missing 'hub_zone' values within each 'hub_name' group.\n    def fill_hub_zone(series):\n        valid = series[~series.isin([\"\", \"unknown\", \"null\", \"nan\", \"n/a\"])]\n        fill_value = valid.iloc[0] if not valid.empty else \"unknown\"\n        return series.fillna(fill_value).replace([\"\", \"unknown\", \"null\", \"nan\", \"n/a\"], fill_value)\n    \n    if 'hub_name' in df.columns and 'hub_zone' in df.columns:\n        if gpu_available:\n            df_pd = df.to_pandas()\n            df_pd['hub_zone'] = df_pd.groupby('hub_name')['hub_zone'].transform(fill_hub_zone)\n            df = cudf.DataFrame.from_pandas(df_pd)\n        else:\n            df = df.shuffle(on=\"hub_name\")\n            df['hub_zone'] = df.groupby('hub_name')['hub_zone'].transform(fill_hub_zone)\n    \n    return df\n\ndef sample_per_hub(df, n=10):\n    \"\"\"\n    Sample up to n rows per 'hub_name' group.\n    \"\"\"\n    def sample_group(pdf, n=n):\n        return pdf.sample(n=n, random_state=42) if len(pdf) > n else pdf\n    if 'hub_name' in df.columns:\n        if gpu_available:\n            df_pd = df.to_pandas()\n            sampled_pd = df_pd.groupby('hub_name').apply(lambda x: sample_group(x, n)).reset_index(drop=True)\n            return sampled_pd\n        else:\n            sampled_ddf = df.groupby('hub_name').apply(sample_group, meta=df._meta)\n            final_df = sampled_ddf.compute()\n            return final_df\n    else:\n        if gpu_available:\n            return df.to_pandas()\n        else:\n            return df.compute()\n\ndef main(main_file, obd_file, output_file):\n    overall_start = time.time()\n    \n    start = time.time()\n    obd_verticals = load_obd_sensitive(obd_file)\n    manual_sensitive = {'pampers', 'diaper'}  # Additional keywords.\n    combined_sensitive = obd_verticals.union(manual_sensitive)\n    print(\"Combined sensitive keywords:\", combined_sensitive)\n    print(\"Time to load OBD sensitive file: {:.2f} seconds\".format(time.time() - start))\n    \n    start = time.time()\n    df = load_main_file(main_file)\n    print(\"Time to load main file: {:.2f} seconds\".format(time.time() - start))\n    \n    start = time.time()\n    df = process_dataframe(df, combined_sensitive)\n    print(\"Time to process dataframe: {:.2f} seconds\".format(time.time() - start))\n    \n    start = time.time()\n    final_df = sample_per_hub(df, n=10)\n    print(\"Time to sample per hub: {:.2f} seconds\".format(time.time() - start))\n    \n    start = time.time()\n    with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n        final_df.to_excel(writer, sheet_name=\"TrackingIDs\", index=False)\n        obd_df = pd.read_excel(obd_file, engine='openpyxl')\n        obd_df.to_excel(writer, sheet_name=\"OBD Sensitive\", index=False)\n    print(\"Time to write Excel: {:.2f} seconds\".format(time.time() - start))\n    \n    overall_time = time.time() - overall_start\n    print(\"Total time taken: {:.2f} seconds\".format(overall_time))\n    print(\"Done. Output written to\", output_file)\n\nif __name__ == '__main__':\n    MAIN_FILE = \"/kaggle/input/demodataset/Week 10 NikhilkrishnanOBD1.xlsx\"  # Give THE Path of input file\n    OBD_FILE = \"/kaggle/input/demodataset/Copy of OBD Sensitive Verticals _ A (1) (1).xlsx\" # Give THE Path of OBD Sensitive file\n    OUTPUT_FILE = \"/kaggle/working/processed_data.xlsx\"\n    main(MAIN_FILE, OBD_FILE, OUTPUT_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T10:17:24.18248Z","iopub.execute_input":"2025-03-11T10:17:24.182796Z","iopub.status.idle":"2025-03-11T10:21:45.876536Z","shell.execute_reply.started":"2025-03-11T10:17:24.182772Z","shell.execute_reply":"2025-03-11T10:21:45.875585Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: XlsxWriter in /usr/local/lib/python3.10/dist-packages (3.2.2)\nGPU acceleration available: Using RAPIDS cuDF\nColumn 'vertical' not found in OBD file. Using the first column as 'vertical'.\nCombined sensitive keywords: {'femaledisorders', 'boysleepwear', 'pleasureenhancement', 'girlblouse', 'womenboxer', 'pampers', 'adultdiapers', 'womenblouse', 'infantbodysuit', 'womenpanty', 'sexualmassager', 'boyinnerwear', 'girlinnerwear', 'womenshirttoptunic', 'menstrunk', 'mensswimsuit', 'pregnancykit', 'womenintimatecare', 'fertilitysupplement', 'piles', 'girlswimsuit', 'tampon', 'menstrualcups', 'womennightdressnighty', 'womenshapewear', 'sexualcomboandkit', 'fertilitykit', 'pantyliner', 'girlbodysuit', 'breastnipplecare', 'girlsleepwear', 'diaper', 'womennightsuit', 'womencamisoleslip', 'womensportbra', 'maledisorders', 'mensbrief', 'infantsleepwear', 'breastpump', 'sexualhealth', 'mensvest', 'sanitarypad', 'womenbabydoll', 'feedingnursing', 'womenlingerieset', 'womenbra', 'womenswimsuit', 'roleplaytoy', 'infantinnerwear', 'mentalwellnessproducts', 'condom', 'babybooty', 'femaleurinationdevice', 'mensboxer'}\nTime to load OBD sensitive file: 0.25 seconds\nReading main XLSX file with pandas and converting to cuDF on GPU...\nTime to load main file: 220.91 seconds\nTotal rows before filtering sensitive content: 667375\nRows after filtering sensitive content: 612774\nTime to process dataframe: 13.63 seconds\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-4-47918b69b495>:158: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  sampled_pd = df_pd.groupby('hub_name').apply(lambda x: sample_group(x, n)).reset_index(drop=True)\n","output_type":"stream"},{"name":"stdout","text":"Time to sample per hub: 5.62 seconds\nTime to write Excel: 17.76 seconds\nTotal time taken: 258.18 seconds\nDone. Output written to /kaggle/working/processed_data.xlsx\n","output_type":"stream"}],"execution_count":4}]}